---
title: "machine_learning"
author: "Marisa Santos"
date: "October 12, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, I will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise.

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

#Preparing the data
In this session we will prepare the data.
```{r}
library(caret)
library(ggplot2)
library(gbm)
library(Amelia)
library(rattle)
library(rpart.plot)
library(rpart)
library(randomForest)
```

##Loading the data
```{r}
data = read.csv("C:/Users/ssmar/Documents/Pos-Graduacao/DataScience/Course08-MachineLearning/final_project/pml-training.csv", sep = ",")

test = read.csv("C:/Users/ssmar/Documents/Pos-Graduacao/DataScience/Course08-MachineLearning/final_project/pml-testing.csv", sep = ",")

```

## Preprocess and clean data
It is necessary remove the missing data and prepare the data to next steps.
```{r}
data[,7:159] <- sapply(data[,7:159],as.numeric) 
test[,7:159] <- sapply(test[,7:159],as.numeric) 

data <- data[8:160]
test <- test[8:160]

sum(is.na(data))
sum(is.na(test))

nas <- is.na(apply(test,2,sum))
test <- test[,!nas]
data <- data[,!nas]

dim(data)
dim(test)

```

##Cross validation
I will use cross validation to split the data variable in two parts: train and val. 
The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset.
Now We have test, train and val variables.
```{r}
inTrain = createDataPartition(y = data$classe, p=0.7, list = F)
train = data[inTrain, ]
val = data[-inTrain, ]
rm(inTrain,nas,data)
```

#Models Predict
In this section, our plan is to build classification tree, random forest, boosting model and bagging for activity classification and then choose the one with the best the out-of-sample accuracy.

##Classification Tree
In the first test, we use a regression tree with the method rpart.
```{r}
set.seed(123)
mod_tree <- train(classe ~ .,data=train, method="rpart")
fancyRpartPlot(mod_tree$finalModel)
pred_tree = predict(mod_tree, val)
conf_tree = confusionMatrix(pred_tree, val$classe)
```

##Random forest
In the second test, we use a random forest with three fold cross validation due the computational cost.
```{r}
mod_rf <- train(classe ~ ., method = "rf", data = train, importance = T, trControl = trainControl(method = "cv", number = 3))
pred_rf = predict(mod_rf, val)
conf_rf = confusionMatrix(pred_rf, val$classe)
plot(mod_rf$finalModel, main = "Random Forest")
```

##Boosting
In the boosting tree model, we use three fold cross-validation.
```{r}
mod_gbm <- train(classe ~ .,data=train, method="gbm", verbose = F, trControl = trainControl(method = "cv", number = 3))
pred_gbm = predict(mod_gbm, val)
conf_gbm = confusionMatrix(pred_gbm, val$classe)
plot(mod_gbm)
```

##Bagging
In the bagging model, we simply use the default setting also we can see the variables most important.
```{r}
mod_bag <-  train(classe ~ .,data=train,method="treebag")
pred_bag = predict(mod_bag, val)
conf_bag  = confusionMatrix(pred_bag, val$classe)
plot(varImp(mod_bag), top = 10)
```

#Choosing the model prediction
Now we are going to select the best model according to accuracy.
```{r}
conf_tree$overall[1]
conf_rf$overall[1]
conf_gbm$overall[1]
conf_bag$overall[1]
```

I will choose the random forest  because the accuracy is `r conf_rf$overall[1]`
```{r}
pred_final = predict(mod_rf, test)
pred_final
```

